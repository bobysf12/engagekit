# Engagement Pipeline + Dashboard Plan

## Scope and Locked Decisions

- Deliver **backend-first**, then UI.
- LLM provider for v1: **OpenRouter**.
- Selection gate for deep scrape: **top 20 by relevance, then score > 75**.
- Draft feedback loop: **store selected option + metadata**, use for **prompt conditioning only** (no model fine-tuning in v1).
- Dashboard backend stack: **Express** API.
- Delete behavior in UI: **hard delete with cascade**.

## Target End-to-End Flow

1. Light scrape (~100 posts/account) from home/profile/search.
2. LLM triage on all light posts with structured output:
   - `relevance_score` (0-100)
   - `relevance_label` (`keep|maybe|drop`)
   - `reasons` (short)
   - `action` (`reply|quote|save|ignore`)
   - `confidence` (0-1)
3. Select top 20, mark deep-scrape subset (`score > 75`).
4. Deep scrape selected posts (comments/context).
5. LLM draft generation (3 options/post) using policy + post + top 3 comments + past approved replies.
6. User picks one option; persist selection as future generation signal.

## Phase Plan

### Phase 1 - Data Model Foundation ✅ COMPLETE

**Status: Completed**

Additive schema changes first (no breaking changes):

- New table: `engagement_policies`
  - per-account current policy (topics, goals, avoid list, tone/identity, preferred languages)
- New table: `engagement_policy_snapshots`
  - frozen policy JSON per `scrape_run_accounts.id`
- New table: `post_triage`
  - per post triage output + rank + selected flags + model/prompt version
- New table: `deep_scrape_tasks`
  - per selected post task state (`pending|running|success|failed`) + retries/error detail
- Extend `llm_drafts`
  - `run_account_id`, `post_id`, `option_index`, `input_context_json`, `selected_at`, `selected_by`
- New table: `draft_feedback_signals`
  - selected draft id + rejected ids + metadata JSON (why chosen, tone fit, intent tag)
- New table: `cron_jobs`
  - account binding, cron expression, timezone, enabled flag, last/next run state
- New table: `cron_job_runs`
  - execution history linked to `scrape_runs`

Indexes/constraints:
- Uniques: `(run_account_id, post_id)` on triage/tasks; `(account_id)` on active policy.
- Query indexes for dashboard filters: run/status/relevance/created_at.

Exit criteria:
- `bun run db:generate` + `bun run db:migrate` succeeds. ✅
- Repositories compile and basic CRUD tests pass. ✅
- All 31 tests pass. ✅

### Phase 2 - LLM Core (OpenRouter) + Policy Snapshot ✅ COMPLETE

**Status: Completed**

Implement LLM service layer and contracts:

- `openrouter-client` with timeout/retry and strict Zod response parsing.
- Prompt modules:
  - triage prompt (structured scoring/action output)
  - draft prompt (3 options with policy + context)
- Policy snapshot service:
  - resolve active account policy
  - persist immutable snapshot at run-account start

Exit criteria:
- Unit tests validate parsing/normalization for malformed model responses. ✅
- Run-account always has exactly one policy snapshot when pipeline starts. ✅

### Phase 3 - Pipeline Orchestration Stages ✅ COMPLETE

**Status: Completed**

Add a post-scrape pipeline coordinator (per run-account):

- Stage A: Triage all light posts for that run-account and persist `post_triage`.
- Stage B: Deterministic selection (top 20, then mark `score > 75` for deep scrape).
- Stage C: Execute deep scrape only for selected subset; persist comments/snapshots/metrics + task status.
- Stage D: Generate 3 drafts per selected post and persist to `llm_drafts`.

Operational behavior:
- Idempotency keys per stage (`run_account_id + post_id`).
- Partial-failure tolerant: failed post-level tasks do not fail whole run.
- Reuse existing `retry` utility for transient LLM/network failures.

Exit criteria:
- Single manual scrape run produces triage rows, selected rows, deep-scrape tasks, and 3 drafts for eligible posts. ✅

### Phase 4 - CLI Extensions for Operability ✅ COMPLETE

**Status: Completed**

Add/extend commands for backend-first management:

- trigger pipeline-aware scrape manually
- list triage results for run-account
- list/select/reject draft options
- CRUD for account engagement policy
- CRUD/list for cron jobs

Exit criteria:
- Full workflow can be run without UI. ✅

### Phase 5 - Express API

Create API server and route modules:

- Runs: list/detail/manual trigger/delete (hard cascade)
- Posts: list/filter/delete (hard cascade)
- Triage: list by run/account/status/threshold
- Drafts: list by post/run, select option, reject option
- Policies: get/update per account
- Cron jobs: create/update/enable-disable/delete/list

Add explicit delete service rules to ensure safe cascade order and transactional integrity in SQLite.

Exit criteria:
- API supports all dashboard actions via integration tests.

### Phase 6 - React/Vite + shadcn Dashboard

Pages:

- Dashboard overview (runs, health, counts)
- Scrape runs list + run detail drilldown
- Scraped posts table (filters + delete)
- Draft review workspace (3 options, choose one, metadata capture)
- Cron jobs management
- Engagement policy editor

UI behavior:
- Server-side filtering/pagination from API.
- Confirm dialogs for hard deletes.
- Clear run/account scoping in all tables.

Exit criteria:
- User can execute end-to-end workflow entirely in UI.

## Critical Files to Modify

- `src/db/schema.ts`
- `src/core/config.ts`
- `src/orchestration/scrape-coordinator.ts`
- `src/orchestration/account-scrape-runner.ts`
- `src/db/repositories/queue.repo.ts`
- `src/cli/index.ts`
- `src/cli/commands/scrape.ts`
- `src/cli/commands/runs.ts`
- `src/cli/commands/queue.ts`

## Critical Files to Add

- `src/db/repositories/engagement-policies.repo.ts`
- `src/db/repositories/engagement-policy-snapshots.repo.ts`
- `src/db/repositories/post-triage.repo.ts`
- `src/db/repositories/deep-scrape-tasks.repo.ts`
- `src/db/repositories/draft-feedback.repo.ts`
- `src/db/repositories/cron-jobs.repo.ts`
- `src/llm/openrouter-client.ts`
- `src/llm/contracts.ts`
- `src/llm/prompts/triage.ts`
- `src/llm/prompts/draft.ts`
- `src/services/policy-snapshot.service.ts`
- `src/orchestration/engagement-pipeline-coordinator.ts`
- `src/orchestration/stages/triage-stage.ts`
- `src/orchestration/stages/selection-stage.ts`
- `src/orchestration/stages/deep-scrape-stage.ts`
- `src/orchestration/stages/draft-generation-stage.ts`
- `src/server/index.ts`
- `src/server/routes/runs.routes.ts`
- `src/server/routes/posts.routes.ts`
- `src/server/routes/triage.routes.ts`
- `src/server/routes/drafts.routes.ts`
- `src/server/routes/policies.routes.ts`
- `src/server/routes/cron.routes.ts`
- `web/` (React/Vite/shadcn app)

## Verification Plan

### Automated

- `bun run typecheck`
- `bun test`
- Add targeted tests for:
  - triage output validation
  - selector logic (top20 + score gate)
  - policy snapshot immutability
  - draft selection feedback persistence
  - hard-delete cascade behavior

### End-to-End Manual (v1)

1. Create/update policy for one account.
2. Trigger manual scrape run.
3. Confirm ~100 light posts captured.
4. Confirm triage rows exist for those posts.
5. Confirm top 20 ranking and `score > 75` deep-scrape picks.
6. Confirm comments/context persisted only for selected posts.
7. Confirm 3 draft options generated per selected post.
8. Choose one draft; verify `draft_feedback_signals` row is written.
9. Delete one post and one run via API/UI; verify dependent rows removed.

### Rollout Strategy

- Gate new stages with env flags (`TRIAGE_ENABLED`, `DEEP_SCRAPE_ENABLED`, `DRAFTS_ENABLED`, `API_ENABLED`, `SCHEDULER_ENABLED`).
- Start with one account/manual trigger only.
- Enable scheduler after stable error rate and data quality checks.
